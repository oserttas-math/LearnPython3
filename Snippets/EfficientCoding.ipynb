{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:07:02.057993Z",
     "start_time": "2019-12-01T19:07:02.054344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Consider generators instead of returning lists\n",
    "\"\"\"This content is based off of Brett Slatkin's Effective Python video lectures\"\"\"\n",
    "def index_words(handle):\n",
    "    offset = 0\n",
    "    for line in handle:\n",
    "        if line:\n",
    "            yield offset\n",
    "        for letter in line:\n",
    "            offset +=1\n",
    "            if letter == ' ':\n",
    "                yield offset\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:08:38.097416Z",
     "start_time": "2019-12-01T19:08:38.094114Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = \"he simplest choice for functions that produce a sequence of results is to return a list of items. For example, say you want to find the index of every word in a string. So, here I'm gonna define such a function. Takes the input texts, then it's going to have the results of the index of each word in that text. I'm assuming the text doesn't have any whitespace before or after it. And so if the text is not empty, an empty string, then we know that the very first position is a word. \"\n",
    "\n",
    "with open('sampletext.txt', 'w') as f:\n",
    "    f.write(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:09:32.405088Z",
     "start_time": "2019-12-01T19:09:32.400070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "with open('sampletext.txt', 'r') as f:\n",
    "    it = index_words(f)\n",
    "    print(next(it))\n",
    "    print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:31:56.933363Z",
     "start_time": "2019-12-01T19:31:56.930103Z"
    }
   },
   "outputs": [],
   "source": [
    "# memory inefficient way of doing this\n",
    "def index_indicator(statement):\n",
    "    result = []\n",
    "    if statement:\n",
    "        result.append(0)\n",
    "    for index, letter in enumerate(statement):\n",
    "        if letter == ' ':\n",
    "            result.append(index+1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T19:34:20.096617Z",
     "start_time": "2019-12-01T19:34:20.091941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sampletext.txt', 'r') as f:\n",
    "    result = index_indicator(f.readline())\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So the second function is using memory to save all indicies of words which could crash program if the text file is huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:00:41.775343Z",
     "start_time": "2019-12-01T21:00:41.770767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.823529411764707, 47.05882352941177, 44.11764705882353]\n"
     ]
    }
   ],
   "source": [
    "data = [15, 80, 75]\n",
    "\n",
    "def normalize(numbers):\n",
    "    total = sum(numbers)\n",
    "    result = []\n",
    "    for value in numbers:\n",
    "        percent = 100 * value / total\n",
    "        result.append(percent)\n",
    "    return result\n",
    "\n",
    "\n",
    "output = normalize(data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:02:35.663378Z",
     "start_time": "2019-12-01T21:02:35.659440Z"
    }
   },
   "outputs": [],
   "source": [
    "# write data to file\n",
    "data = [15, 80, 75, 90, 12, 45]\n",
    "with open('numbers_data.txt', 'w') as f:\n",
    "    for i in data:\n",
    "        f.write('{}\\n'.format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:04:01.290231Z",
     "start_time": "2019-12-01T21:04:01.287113Z"
    }
   },
   "outputs": [],
   "source": [
    "# open data file with a generator\n",
    "def read_numbers(path):\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            yield int(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:06:47.629851Z",
     "start_time": "2019-12-01T21:06:47.624888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object read_numbers at 0x10aa60ed0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_numbers(path='numbers_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:07:19.940752Z",
     "start_time": "2019-12-01T21:07:19.936416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 80, 75, 90, 12, 45]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(read_numbers(path='numbers_data.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:09:27.385355Z",
     "start_time": "2019-12-01T21:09:27.381935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# now let us see how iterator will behave\n",
    "data_list = read_numbers(path='numbers_data.txt')\n",
    "print(normalize(data_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cause of this behavior is that an iterator only produces its results a single time and if you iterate over an iterator or a generator that's already raised a stop iteration exception, you won't get any results the second time around.\n",
    "- If we run it a second time, you'll see that there's no exception raised. There's no obvious problem here. But the list that's created on the second time through is just empty. And if we do it, you know, multiple times after that, again we just get empty lists. So the problem here is that functions like list can't tell the difference between an iterator that has no output at all and an iterator that had output before like the first time through this loop, and that output's now exhausted. The stop iteration exception happens in both cases, and it's considered like normal operation for Python. And so the system can't tell the difference. And this same behavior will affect for loops and many other functions throughout the Python standard library that are expecting the stop iteration exception. To solve this problem, you can explicitly exhaust an input iterator and keep a copy of its entire contents in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:12:50.725832Z",
     "start_time": "2019-12-01T21:12:50.722061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 80, 75, 90, 12, 45]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "iterator = read_numbers(path='numbers_data.txt')\n",
    "print(list(iterator))\n",
    "print(list(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So let's change this function that we had before to avoid this issue of multiple iterators. Now let's take the numbers in input and let's redefine it as a list of the same value. And what that's doing is copying the iterator. So then later when we go through this and sum the numbers, we're iterating through a copy of all the numbers, we know they're all there. The second time through we're doing that again. Again we know it's a list so that it can't possibly be exhausted. And so we'll properly see everything that's there. Now if we have the read visits function again and we run, we run it on the path we had from before. And we pass that iterator into the normalize function, we'll see that this time around, it works properly. So because we added this copy up at the top, the normalize function will correctly work on an iterator, the iterator version of the input. So we've avoided the problem now. Unfortunately, it's not that easy. We've solved one problem but we've created a whole other problem. So the other problem here is that the list of numbers that we created up here to prevent the iterator exhaustion, that's going to create a copy of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:36:44.529855Z",
     "start_time": "2019-12-01T21:36:44.525136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.73186119873817, 25.236593059936908, 23.65930599369085, 28.391167192429023, 3.7854889589905363, 14.195583596214512]\n"
     ]
    }
   ],
   "source": [
    "# solution\n",
    "def normalize(numbers):\n",
    "    numbers = list(numbers) # copy the iterator\n",
    "    total = sum(numbers)\n",
    "    result = []\n",
    "    for value in numbers:\n",
    "        percent = 100 * value / total\n",
    "        result.append(percent)\n",
    "    return result\n",
    "\n",
    "it = read_numbers('numbers_data.txt')\n",
    "print(normalize(it))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  So we really haven't fundamentally solved the issue here, we've just made it not have the surprising behavior. Now the big difference between this version and the version before, pasting back in how we did it before, we can't just pass an iterator into normalize anymore. Because we actually need a function that generates this iterator. And so the way to actually do this is to create a new lambda function which returns back the iterator from read visits. And then we pass this iterator into normalize. And if we do this, then we get the correct behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:44:49.390056Z",
     "start_time": "2019-12-01T21:44:49.385211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317\n",
      "[4.73186119873817, 25.236593059936908, 23.65930599369085, 28.391167192429023, 3.7854889589905363, 14.195583596214512]\n"
     ]
    }
   ],
   "source": [
    "# another way\n",
    "def normalize(numbers):\n",
    "    total = sum(get_iter()) # New iterator\n",
    "    print(total)\n",
    "    result = []\n",
    "    for value in get_iter(): # New iterator\n",
    "        percent = 100 * value / total\n",
    "        result.append(percent)\n",
    "    return result\n",
    "\n",
    "get_iter = lambda: read_numbers('numbers_data.txt')\n",
    "print(normalize(get_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The way this works is, up here in normalize, get iterator is going to be called in both of these cases. That ends up evaluating this lambda. That lambda is going to call **read_numbers** multiple times. Twice, one for each time get iter is called. And so we get the correct behavior, but the natural outcome here is that we call revisits twice. Which means we've opened the data file and read the entire data file twice. So you're doing two passes through the entire input data which is the downside here. The upside is it can handle any amount of memory that the file could possibly contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:45:20.609205Z",
     "start_time": "2019-12-01T21:45:20.605422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>()>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda: read_numbers('numbers_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-01T21:58:39.049807Z",
     "start_time": "2019-12-01T21:58:39.042511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.73186119873817, 25.236593059936908, 23.65930599369085, 28.391167192429023, 3.7854889589905363, 14.195583596214512]\n"
     ]
    }
   ],
   "source": [
    "# better way for Pythonista\n",
    "class ReadNumbers(object):\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with open(self.data_path) as f:\n",
    "            for line in f:\n",
    "                yield int(line)\n",
    "                \n",
    "visits  = ReadNumbers('numbers_data.txt')\n",
    "\n",
    "\n",
    "def normalize(numbers):\n",
    "    total = sum(numbers) # this will iterate over visits to compete sum\n",
    "    result = []\n",
    "    for value in numbers:\n",
    "        percent = 100 * value / total\n",
    "        result.append(percent)\n",
    "    return result\n",
    "\n",
    "\n",
    "print(normalize(visits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why does this work where it didn't work before? Well, what happens here is the visits objects gets passed into normalize. Normalize is going to iterate over the numbers function multiple times. Each time it tries to iterate over the numbers object, say with the sum built in function. What sum is actually going to try to do is say \"Okay, hey, this is numbers. \"I need to iterate over this. \"I'm going to call the iter built in function.\" By calling the iter built in function, it calls the read visits iter method. And the read visits iter method is going to open the data file a whole separate time and then give you all of the data as a generator in return. That let's the sum built in function here open the file, create a new iterator, iterate through the whole thing, exhaust that iterator, and then get the total. Then further down in the function for this for loop, the numbers where we loop through the numbers again, this for loop is going to call iter on numbers a second time. And by calling iter on numbers a second time, this method gets called a second time, which then allocates this iterator, or opens this file a second time, and then causes a generation of all those line numbers a second time. So this works because we actually do two passes through the data. But from the standpoint from the function, it has no idea. It just knows that it's going to be passed in a container of numbers, and it's going to try to iterate over that container multiple times. And it's going to expect that iterating multiple times is going to work just like it would with a standard Python list. Now that you know how containers like read visits work, you can write your own functions to ensure that parameters like number in this normalize function aren't just iterators that are going to cause this buggy behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
